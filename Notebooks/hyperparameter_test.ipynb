{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afeb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "here = Path.cwd()\n",
    "candidates = [here] + list(here.parents)\n",
    "for p in candidates:\n",
    "    if (p / \"Code\").is_dir():\n",
    "        sys.path.insert(0, str(p))\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Couldn't find a 'Code' folder in this project.\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from Code.PINN import create_network_model, u, compute_loss, g_trial_tf, make_train_step, compute_MSE\n",
    "from Code.functions import euler, analytical_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb37aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 11:59:13.555610: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-12-12 11:59:13.555646: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-12-12 11:59:13.555652: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.92 GB\n",
      "2025-12-12 11:59:13.555688: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-12-12 11:59:13.555702: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "N_points = 1000  # Number of collocation points (x, t) in the domain\n",
    "epochs = 3000  # Number of training epochs\n",
    "\n",
    "T_final = 0.5 # Example final time\n",
    "x_samples = np.random.uniform(0.0, 1.0, N_points)\n",
    "t_samples = np.random.uniform(0.0, T_final, N_points)\n",
    "X_train = np.stack([x_samples, t_samples], axis=1).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32add72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Width: 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 11:59:14.133963: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 0: loss = 2.017e+01\n",
      "  Epoch 500: loss = 7.919e+00\n",
      "  Epoch 1000: loss = 7.672e+00\n",
      "  Epoch 1500: loss = 1.559e+00\n",
      "  Epoch 2000: loss = 1.118e+00\n",
      "  Epoch 2500: loss = 8.975e-01\n",
      "Final residual loss: 7.639e-01, MSE vs exact: 1.030e-03\n",
      "\n",
      "=== Width: 10 ===\n",
      "  Epoch 0: loss = 2.127e+01\n",
      "  Epoch 500: loss = 1.051e+00\n",
      "  Epoch 1000: loss = 2.671e-01\n",
      "  Epoch 1500: loss = 1.205e-01\n",
      "  Epoch 2000: loss = 7.406e-02\n",
      "  Epoch 2500: loss = 5.223e-02\n",
      "Final residual loss: 3.752e-02, MSE vs exact: 1.893e-05\n",
      "\n",
      "=== Width: 20 ===\n",
      "  Epoch 0: loss = 2.307e+01\n",
      "  Epoch 500: loss = 8.676e-01\n",
      "  Epoch 1000: loss = 1.745e-01\n",
      "  Epoch 1500: loss = 5.341e-02\n",
      "  Epoch 2000: loss = 2.510e-02\n",
      "  Epoch 2500: loss = 1.379e-02\n",
      "Final residual loss: 9.368e-03, MSE vs exact: 2.246e-06\n",
      "\n",
      "=== Width: 50 ===\n",
      "  Epoch 0: loss = 2.298e+01\n",
      "  Epoch 500: loss = 5.327e-01\n",
      "  Epoch 1000: loss = 4.198e-02\n",
      "  Epoch 1500: loss = 1.792e-02\n",
      "  Epoch 2000: loss = 9.757e-03\n",
      "  Epoch 2500: loss = 4.999e-03\n",
      "Final residual loss: 2.851e-03, MSE vs exact: 1.039e-06\n",
      "\n",
      "=== Width: 100 ===\n",
      "  Epoch 0: loss = 2.085e+01\n",
      "  Epoch 500: loss = 2.298e-01\n",
      "  Epoch 1000: loss = 1.926e-02\n",
      "  Epoch 1500: loss = 9.393e-03\n",
      "  Epoch 2000: loss = 5.803e-03\n",
      "  Epoch 2500: loss = 3.969e-03\n",
      "Final residual loss: 2.881e-03, MSE vs exact: 2.642e-07\n",
      "   Width  Residual loss           MSE\n",
      "0      5       0.763870  1.030024e-03\n",
      "1     10       0.037524  1.893246e-05\n",
      "2     20       0.009368  2.245939e-06\n",
      "3     50       0.002851  1.038812e-06\n",
      "4    100       0.002881  2.641951e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "widths = [5, 10, 20, 50, 100]\n",
    "results_nodes = []\n",
    "\n",
    "epochs = 3000 \n",
    "\n",
    "for w in widths:\n",
    "    print(f\"\\n=== Width: {w} ===\")\n",
    "    \n",
    "    model = create_network_model(layers=[w], activation='tanh')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    train_step = make_train_step(model, optimizer, compute_loss)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(X_train)\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"  Epoch {epoch}: loss = {loss.numpy():.3e}\")\n",
    "\n",
    "    mse = compute_MSE(model, T_final=T_final)\n",
    "    results_nodes.append((w, float(loss.numpy()), mse))\n",
    "    print(f\"Final residual loss: {loss.numpy():.3e}, MSE vs exact: {mse:.3e}\")\n",
    "\n",
    "df_nodes = pd.DataFrame(results_nodes, columns=[\"Width\", \"Residual loss\", \"MSE\"])\n",
    "print(df_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9829a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Depth experiment: layers=[50], tanh ===\n",
      "  Epoch 0: loss = 2.046e+01\n",
      "  Epoch 500: loss = 6.492e-01\n",
      "  Epoch 1000: loss = 4.560e-02\n",
      "  Epoch 1500: loss = 1.680e-02\n",
      "  Epoch 2000: loss = 9.169e-03\n",
      "  Epoch 2500: loss = 5.333e-03\n",
      "Final residual loss: 4.394e-03, MSE vs exact: 7.922e-06\n",
      "\n",
      "=== Depth experiment: layers=[50, 50], tanh ===\n",
      "  Epoch 0: loss = 2.015e+01\n",
      "  Epoch 500: loss = 6.615e-03\n",
      "  Epoch 1000: loss = 3.569e-02\n",
      "  Epoch 1500: loss = 5.640e-02\n",
      "  Epoch 2000: loss = 2.394e-04\n",
      "  Epoch 2500: loss = 3.857e-04\n",
      "Final residual loss: 1.826e-04, MSE vs exact: 7.792e-08\n",
      "\n",
      "=== Depth experiment: layers=[50, 50, 50], tanh ===\n",
      "  Epoch 0: loss = 2.305e+01\n",
      "  Epoch 500: loss = 4.001e-03\n",
      "  Epoch 1000: loss = 7.092e-04\n",
      "  Epoch 1500: loss = 3.173e-04\n",
      "  Epoch 2000: loss = 3.203e-04\n",
      "  Epoch 2500: loss = 9.808e-03\n",
      "Final residual loss: 1.263e-02, MSE vs exact: 6.995e-05\n",
      "\n",
      "=== Depth experiment: layers=[50, 50, 50, 50], tanh ===\n",
      "  Epoch 0: loss = 2.213e+01\n",
      "  Epoch 500: loss = 6.625e-03\n",
      "  Epoch 1000: loss = 8.971e-04\n",
      "  Epoch 1500: loss = 4.424e-02\n",
      "  Epoch 2000: loss = 2.183e-04\n",
      "  Epoch 2500: loss = 1.098e-03\n",
      "Final residual loss: 6.173e-04, MSE vs exact: 1.467e-06\n",
      "       Architecture  Residual loss           MSE\n",
      "0              [50]       0.004394  7.922110e-06\n",
      "1          [50, 50]       0.000183  7.792142e-08\n",
      "2      [50, 50, 50]       0.012630  6.995388e-05\n",
      "3  [50, 50, 50, 50]       0.000617  1.467221e-06\n"
     ]
    }
   ],
   "source": [
    "depth_architectures = [\n",
    "    [50],\n",
    "    [50, 50],\n",
    "    [50, 50, 50],\n",
    "    [50, 50, 50, 50],\n",
    "]\n",
    "\n",
    "results_layers = []\n",
    "\n",
    "for layers in depth_architectures:\n",
    "    print(f\"\\n=== Depth experiment: layers={layers}, tanh ===\")\n",
    "    # Build new model\n",
    "    model = create_network_model(layers=layers)\n",
    "    \n",
    "    # IMPORTANT: new optimizer for each model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Reset traced graph: DON'T reuse old tf.function trace\n",
    "    train_step = make_train_step(model, optimizer, compute_loss)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(X_train)\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"  Epoch {epoch}: loss = {loss.numpy():.3e}\")\n",
    "\n",
    "    mse = compute_MSE(model, T_final=T_final)\n",
    "    results_layers.append((str(layers), float(loss.numpy()), mse))\n",
    "    print(f\"Final residual loss: {loss.numpy():.3e}, MSE vs exact: {mse:.3e}\")\n",
    "\n",
    "df_layers = pd.DataFrame(results_layers, columns=[\"Architecture\", \"Residual loss\", \"MSE\"])\n",
    "print(df_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aead507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Activation experiment: tanh, layers=[50,50] ===\n",
      "  Epoch 0: loss = 2.220e+01\n",
      "  Epoch 500: loss = 6.032e-03\n",
      "  Epoch 1000: loss = 9.157e-04\n",
      "  Epoch 1500: loss = 4.217e-04\n",
      "  Epoch 2000: loss = 2.431e-03\n",
      "  Epoch 2500: loss = 1.421e-03\n",
      "Final residual loss: 1.727e-04, MSE vs exact: 5.609e-08\n",
      "\n",
      "=== Activation experiment: sigmoid, layers=[50,50] ===\n",
      "  Epoch 0: loss = 1.738e+01\n",
      "  Epoch 500: loss = 7.475e-02\n",
      "  Epoch 1000: loss = 6.203e-03\n",
      "  Epoch 1500: loss = 1.798e-03\n",
      "  Epoch 2000: loss = 7.606e-04\n",
      "  Epoch 2500: loss = 4.049e-04\n",
      "Final residual loss: 2.542e-04, MSE vs exact: 3.463e-08\n",
      "\n",
      "=== Activation experiment: relu, layers=[50,50] ===\n",
      "  Epoch 0: loss = 2.031e+01\n",
      "  Epoch 500: loss = 1.220e+00\n",
      "  Epoch 1000: loss = 3.752e+00\n",
      "  Epoch 1500: loss = 2.063e+00\n",
      "  Epoch 2000: loss = 2.335e+00\n",
      "  Epoch 2500: loss = 2.392e+00\n",
      "Final residual loss: 2.308e+00, MSE vs exact: 1.037e+01\n",
      "\n",
      "=== Activation experiment: swish, layers=[50,50] ===\n",
      "  Epoch 0: loss = 2.119e+01\n",
      "  Epoch 500: loss = 3.703e-03\n",
      "  Epoch 1000: loss = 1.561e-03\n",
      "  Epoch 1500: loss = 1.087e-03\n",
      "  Epoch 2000: loss = 2.113e-04\n",
      "  Epoch 2500: loss = 1.351e-04\n",
      "Final residual loss: 8.589e-05, MSE vs exact: 7.922e-09\n",
      "  Activation  Residual loss           MSE\n",
      "0       tanh       0.000173  5.609171e-08\n",
      "1    sigmoid       0.000254  3.463482e-08\n",
      "2       relu       2.308150  1.036504e+01\n",
      "3      swish       0.000086  7.921836e-09\n"
     ]
    }
   ],
   "source": [
    "activations = ['tanh', 'sigmoid', 'relu', 'swish']\n",
    "\n",
    "results_activations = []\n",
    "epochs = 3000\n",
    "\n",
    "for act in activations:\n",
    "    print(f\"\\n=== Activation experiment: {act}, layers=[50,50] ===\")\n",
    "\n",
    "    # 1. Create model with the activation\n",
    "    model = create_network_model(layers=[50, 50], activation=act)\n",
    "\n",
    "    # 2. New optimizer per model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    # 3. Create a NEW train_step bound to this model & optimizer\n",
    "    train_step = make_train_step(model, optimizer, compute_loss)\n",
    "\n",
    "    # 4. Training loop\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(X_train)\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"  Epoch {epoch}: loss = {loss.numpy():.3e}\")\n",
    "\n",
    "    # 5. Compute accuracy\n",
    "    mse = compute_MSE(model, T_final=T_final)\n",
    "\n",
    "    results_activations.append((act, float(loss.numpy()), mse))\n",
    "\n",
    "    print(f\"Final residual loss: {loss.numpy():.3e}, MSE vs exact: {mse:.3e}\")\n",
    "\n",
    "df_act = pd.DataFrame(results_activations, columns=[\"Activation\", \"Residual loss\", \"MSE\"])\n",
    "print(df_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1530c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Learning rate experiment: eta=0.0001, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.124e+01\n",
      "  Epoch 500: loss = 1.401e+01\n",
      "  Epoch 1000: loss = 1.050e+01\n",
      "  Epoch 1500: loss = 9.596e+00\n",
      "  Epoch 2000: loss = 8.882e+00\n",
      "  Epoch 2500: loss = 7.953e+00\n",
      "  Epoch 3000: loss = 6.453e+00\n",
      "  Epoch 3500: loss = 4.280e+00\n",
      "  Epoch 4000: loss = 2.212e+00\n",
      "  Epoch 4500: loss = 1.244e+00\n",
      "Final residual loss: 1.047e+00, MSE vs exact: 2.189e-03\n",
      "\n",
      "=== Learning rate experiment: eta=0.001, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.132e+01\n",
      "  Epoch 500: loss = 1.129e+00\n",
      "  Epoch 1000: loss = 1.074e-01\n",
      "  Epoch 1500: loss = 2.042e-02\n",
      "  Epoch 2000: loss = 8.716e-03\n",
      "  Epoch 2500: loss = 5.091e-03\n",
      "  Epoch 3000: loss = 3.108e-03\n",
      "  Epoch 3500: loss = 1.719e-03\n",
      "  Epoch 4000: loss = 8.770e-04\n",
      "  Epoch 4500: loss = 4.837e-04\n",
      "Final residual loss: 3.095e-04, MSE vs exact: 4.653e-08\n",
      "\n",
      "=== Learning rate experiment: eta=0.01, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.149e+01\n",
      "  Epoch 500: loss = 3.847e-03\n",
      "  Epoch 1000: loss = 7.912e-04\n",
      "  Epoch 1500: loss = 2.525e-04\n",
      "  Epoch 2000: loss = 1.660e-04\n",
      "  Epoch 2500: loss = 1.173e-04\n",
      "  Epoch 3000: loss = 9.232e-05\n",
      "  Epoch 3500: loss = 6.398e-03\n",
      "  Epoch 4000: loss = 6.124e-05\n",
      "  Epoch 4500: loss = 5.733e-05\n",
      "Final residual loss: 3.858e-05, MSE vs exact: 2.787e-09\n",
      "\n",
      "=== Learning rate experiment: eta=0.1, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.073e+01\n",
      "  Epoch 500: loss = 9.822e-03\n",
      "  Epoch 1000: loss = 3.006e-03\n",
      "  Epoch 1500: loss = 1.667e-03\n",
      "  Epoch 2000: loss = 1.202e-03\n",
      "  Epoch 2500: loss = 6.057e-04\n",
      "  Epoch 3000: loss = 2.420e-04\n",
      "  Epoch 3500: loss = 3.413e-04\n",
      "  Epoch 4000: loss = 1.897e-04\n",
      "  Epoch 4500: loss = 3.894e-03\n",
      "Final residual loss: 2.848e-03, MSE vs exact: 6.929e-06\n",
      "\n",
      "=== Learning rate experiment: eta=1.0, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.096e+01\n",
      "  Epoch 500: loss = 6.399e+00\n",
      "  Epoch 1000: loss = 5.432e+00\n",
      "  Epoch 1500: loss = 4.965e+00\n",
      "  Epoch 2000: loss = 4.534e+00\n",
      "  Epoch 2500: loss = 4.172e+00\n",
      "  Epoch 3000: loss = 3.772e+00\n",
      "  Epoch 3500: loss = 3.375e+00\n",
      "  Epoch 4000: loss = 3.775e+00\n",
      "  Epoch 4500: loss = 8.789e+00\n",
      "Final residual loss: 8.789e+00, MSE vs exact: 4.104e-02\n",
      "   Learning rate  Residual loss           MSE\n",
      "0         0.0001       1.047014  2.188606e-03\n",
      "1         0.0010       0.000310  4.652575e-08\n",
      "2         0.0100       0.000039  2.786933e-09\n",
      "3         0.1000       0.002848  6.929239e-06\n",
      "4         1.0000       8.788813  4.104415e-02\n"
     ]
    }
   ],
   "source": [
    "eta = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "epochs = 5000\n",
    "\n",
    "results_eta = []\n",
    "\n",
    "for learning_rate in eta:\n",
    "    print(f\"\\n=== Learning rate experiment: eta={learning_rate}, layers=[50,50], swish ===\")\n",
    "\n",
    "    # 1. Create model\n",
    "    model = create_network_model(layers=[50, 50], activation='swish')\n",
    "\n",
    "    # 2. New optimizer per model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # 3. Create a NEW train_step bound to this model & optimizer\n",
    "    train_step = make_train_step(model, optimizer, compute_loss)\n",
    "\n",
    "    # 4. Training loop\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(X_train)\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"  Epoch {epoch}: loss = {loss.numpy():.3e}\")\n",
    "\n",
    "    # 5. Compute accuracy\n",
    "    mse = compute_MSE(model, T_final=T_final)\n",
    "\n",
    "    results_eta.append((learning_rate, float(loss.numpy()), mse))\n",
    "\n",
    "    print(f\"Final residual loss: {loss.numpy():.3e}, MSE vs exact: {mse:.3e}\")\n",
    "    \n",
    "df_eta = pd.DataFrame(results_eta, columns=[\"Learning rate\", \"Residual loss\", \"MSE\"])\n",
    "print(df_eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af69cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed experiment: seed=0, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.102e+01\n",
      "  Epoch 500: loss = 1.139e+00\n",
      "  Epoch 1000: loss = 1.578e-01\n",
      "  Epoch 1500: loss = 3.358e-02\n",
      "  Epoch 2000: loss = 1.353e-02\n",
      "  Epoch 2500: loss = 6.635e-03\n",
      "  Epoch 3000: loss = 4.002e-03\n",
      "  Epoch 3500: loss = 2.418e-03\n",
      "  Epoch 4000: loss = 1.478e-03\n",
      "  Epoch 4500: loss = 9.980e-04\n",
      "Final residual loss: 7.295e-04, MSE vs exact: 1.365e-07\n",
      "\n",
      "=== Seed experiment: seed=1, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.120e+01\n",
      "  Epoch 500: loss = 1.255e+00\n",
      "  Epoch 1000: loss = 4.161e-01\n",
      "  Epoch 1500: loss = 3.060e-02\n",
      "  Epoch 2000: loss = 1.097e-02\n",
      "  Epoch 2500: loss = 5.415e-03\n",
      "  Epoch 3000: loss = 2.738e-03\n",
      "  Epoch 3500: loss = 1.391e-03\n",
      "  Epoch 4000: loss = 7.983e-04\n",
      "  Epoch 4500: loss = 5.161e-04\n",
      "Final residual loss: 3.504e-04, MSE vs exact: 5.387e-08\n",
      "\n",
      "=== Seed experiment: seed=2, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.105e+01\n",
      "  Epoch 500: loss = 1.125e+00\n",
      "  Epoch 1000: loss = 2.493e-01\n",
      "  Epoch 1500: loss = 3.916e-02\n",
      "  Epoch 2000: loss = 1.343e-02\n",
      "  Epoch 2500: loss = 6.467e-03\n",
      "  Epoch 3000: loss = 3.836e-03\n",
      "  Epoch 3500: loss = 2.425e-03\n",
      "  Epoch 4000: loss = 1.539e-03\n",
      "  Epoch 4500: loss = 9.779e-04\n",
      "Final residual loss: 6.257e-04, MSE vs exact: 7.747e-08\n",
      "\n",
      "=== Seed experiment: seed=3, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.097e+01\n",
      "  Epoch 500: loss = 1.134e+00\n",
      "  Epoch 1000: loss = 1.745e-01\n",
      "  Epoch 1500: loss = 2.685e-02\n",
      "  Epoch 2000: loss = 1.104e-02\n",
      "  Epoch 2500: loss = 6.534e-03\n",
      "  Epoch 3000: loss = 4.282e-03\n",
      "  Epoch 3500: loss = 2.622e-03\n",
      "  Epoch 4000: loss = 1.462e-03\n",
      "  Epoch 4500: loss = 8.189e-04\n",
      "Final residual loss: 5.031e-04, MSE vs exact: 8.771e-08\n",
      "\n",
      "=== Seed experiment: seed=4, layers=[50,50], swish ===\n",
      "  Epoch 0: loss = 2.084e+01\n",
      "  Epoch 500: loss = 1.074e+00\n",
      "  Epoch 1000: loss = 1.363e-01\n",
      "  Epoch 1500: loss = 2.449e-02\n",
      "  Epoch 2000: loss = 9.753e-03\n",
      "  Epoch 2500: loss = 5.343e-03\n",
      "  Epoch 3000: loss = 3.404e-03\n",
      "  Epoch 3500: loss = 2.140e-03\n",
      "  Epoch 4000: loss = 1.312e-03\n",
      "  Epoch 4500: loss = 8.243e-04\n",
      "Final residual loss: 5.101e-04, MSE vs exact: 7.600e-08\n",
      "   Seed  Residual loss           MSE\n",
      "0     0       0.000729  1.364736e-07\n",
      "1     1       0.000350  5.387183e-08\n",
      "2     2       0.000626  7.747233e-08\n",
      "3     3       0.000503  8.770814e-08\n",
      "4     4       0.000510  7.599761e-08\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "epochs = 5000\n",
    "\n",
    "results_seeds = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Seed experiment: seed={seed}, layers=[50,50], swish ===\")\n",
    "\n",
    "    # --- 1. Set seed BEFORE building the model ---\n",
    "    set_seed(seed)\n",
    "\n",
    "    # --- 2. Create model ---\n",
    "    model = create_network_model(layers=[50, 50], activation='swish')\n",
    "\n",
    "    # --- 3. Optimizer ---\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # --- 4. New train_step for this model ---\n",
    "    train_step = make_train_step(model, optimizer, compute_loss)\n",
    "\n",
    "    # --- 5. Training loop ---\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_step(X_train)\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"  Epoch {epoch}: loss = {loss.numpy():.3e}\")\n",
    "\n",
    "    # --- 6. Compute accuracy ---\n",
    "    mse = compute_MSE(model, T_final=T_final)\n",
    "\n",
    "    results_seeds.append((seed, float(loss.numpy()), mse))\n",
    "\n",
    "    print(f\"Final residual loss: {loss.numpy():.3e}, MSE vs exact: {mse:.3e}\")\n",
    "\n",
    "df_seeds = pd.DataFrame(results_seeds, columns=[\"Seed\", \"Residual loss\", \"MSE\"])\n",
    "print(df_seeds)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_tf)",
   "language": "python",
   "name": "venv_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
